{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ba6f59-3263-4ee3-b1a3-6c4aa5936a45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76da542-c82c-44f7-a9d5-f50050f9bdc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Employee Data\n",
    "employees = [\n",
    "    (1, 'Alice Johnson', 'Engineering', 85000, '2020-01-15', 28),\n",
    "    (2, 'Bob Smith', 'Engineering', 92000, '2019-03-10', 35),\n",
    "    (3, 'Charlie Brown', 'Sales', 65000, '2021-06-01', 26),\n",
    "    (4, 'Diana Prince', 'Engineering', 78000, '2022-02-20', 29),\n",
    "    (5, 'Eve Wilson', 'Marketing', 70000, '2020-08-15', 31),\n",
    "    (6, 'Frank Miller', 'Sales', 72000, '2019-11-30', 42),\n",
    "    (7, 'Grace Lee', 'Engineering', 95000, '2018-05-12', 33),\n",
    "    (8, 'Henry Davis', 'Marketing', 68000, '2021-09-05', 27),\n",
    "    (9, 'Iris Taylor', 'Sales', 58000, '2022-01-10', 24),\n",
    "    (10, 'Jack Wilson', 'Engineering', 88000, '2020-12-01', 30)\n",
    "]\n",
    "\n",
    "#e=spark.createDataFrame(employees, schema=['id', 'name', 'department', 'salary', 'start_date', 'age'])\n",
    "r=e.filter(e.salary > 80000).filter(e.department == 'Engineering').orderBy(F.desc(e.salary)).show()\n",
    "\n",
    "e.createOrReplaceTempView(\"employees\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM employees \n",
    "    WHERE salary > 80000 \n",
    "      AND department = 'Engineering'\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbbf41d3-f0b6-4a26-bdc6-673523bd93ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t = e.groupBy(\"department\") \\\n",
    "    .agg(F.avg('salary').alias('avg_salary'),      # ← No brackets, just commas\n",
    "         F.count('*').alias('emp_count'),          # ← Also changed to count('*')\n",
    "         F.min('salary').alias('min_salary'),\n",
    "         F.max('salary').alias('max_salary')) \\\n",
    "    .filter(F.col('emp_count') > 2) \\\n",
    "    .select('department', 'avg_salary', 'emp_count', 'min_salary', 'max_salary')\n",
    "t.show()\n",
    "\n",
    "t2=spark.sql(\"\"\"\n",
    "    SELECT department, AVG(salary) AS avg_salary, COUNT(*) AS emp_count, MIN(salary) AS min_salary, MAX(salary) AS max_salary\n",
    "    FROM employees \n",
    "    GROUP BY department\n",
    "    HAVING COUNT(*)>2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc09ba86-9942-4deb-8698-3196c6334eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Department Data  \n",
    "departments = [\n",
    "    ('Engineering', 'Tech', 'John Manager', 50),\n",
    "    ('Sales', 'Business', 'Sarah Director', 25), \n",
    "    ('Marketing', 'Business', 'Mike Lead', 15)\n",
    "]\n",
    "# d=spark.createDataFrame(departments, schema=['dept_name', 'category', 'manager', 'team_size'])\n",
    "# d.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "t3= e.join(d, e.department==d.dept_name)\\\n",
    "    .filter(d.category=='Business')\\\n",
    "    .withColumn('salary_level', F.when(F.col('salary')>80000, 'High').when(F.col('salary')>=60000, 'Medium').otherwise('Low'))\\\n",
    "    .withColumn('team_size_cat', F.when(F.col('team_size')>30, 'Large').otherwise('Small'))\\\n",
    "    .select(\"name\", \"salary\", \"manager\", \"salary_level\", \"team_size_cat\")\n",
    "t3.show()\n",
    "\n",
    "s3 = \"\"\"\n",
    "SELECT e.name AS name, e.salary AS salary, d.manager AS manager, \n",
    "CASE WHEN e.salary>80000 THEN \"High\",\n",
    "     WHEN e.salary>=60000 THEN \"Medium\"\n",
    "     ELSE \"Low\" END AS salary_level,\n",
    "CASE WHEN d.team_size>30 THEN \"Large\"\n",
    "     ELSE 'Small' END AS team_size_cat,\n",
    "FROM employees e\n",
    "JOIN departments d ON e.department=d.dept_name\n",
    "WHERE d.category='Business'\n",
    "\"\"\"\n",
    "ts3= spark.sql(s3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0684d898-6eb8-4c92-9859-ed33283bc618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w4 = Window.partitionBy('department').orderBy(F.desc('salary'))\n",
    "t4 = e.withColumn('rank', F.rank().over(w4))\\\n",
    "    .filter(F.col('rank')<=2)\\\n",
    "        .select('name', 'department', 'salary', 'rank')\n",
    "#t4.show()\n",
    "\n",
    "s4 = \"\"\"\n",
    "WITH ranked_employees AS (\n",
    "    SELECT name, \n",
    "           department,\n",
    "           salary,\n",
    "           ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rank\n",
    "    FROM employees\n",
    ")\n",
    "SELECT name, department, salary, rank\n",
    "FROM ranked_employees  \n",
    "WHERE rank <= 2\n",
    "ORDER BY department, rank;\n",
    "\"\"\"\n",
    "ts4=spark.sql(s4)\n",
    "ts4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f819e408-424b-4c6d-9c7e-127d896b7b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_data = [\n",
    "    (1, 'Alice Johnson', '2024-01-15', 15000, 'Software'),\n",
    "    (2, 'Bob Smith', '2024-01-16', 22000, 'Consulting'), \n",
    "    (3, 'Charlie Brown', '2024-01-15', 8000, 'Software'),\n",
    "    (4, 'Alice Johnson', '2024-01-20', 18000, 'Consulting'),\n",
    "    (5, 'Diana Prince', '2024-01-18', 12000, 'Software'),\n",
    "    (6, 'Charlie Brown', '2024-01-22', 25000, 'Consulting')\n",
    "]\n",
    "\n",
    "# s = spark.createDataFrame(sales_data, schema=['sale_id', 'salesperson', 'date', 'amount', 'product_type'])\n",
    "# s.createOrReplaceTempView('sales')\n",
    "tw5 = s.groupBy('salesperson', 'product_type') \\\n",
    "    .agg(F.sum('amount').alias('s_total')) \\\n",
    "    .withColumn('rank', F.row_number().over(Window.partitionBy('salesperson').orderBy(F.desc('s_total')))) \\\n",
    "    .filter(F.col('rank')==1) \\\n",
    "    .select(F.col('salesperson').alias('sperson'), 'product_type', 's_total', 'rank')\n",
    "#tw5.show()\n",
    "\n",
    "tw55= s.groupBy('salesperson') \\\n",
    "    .agg(F.sum('amount').alias('total'),\n",
    "         F.count('*').alias('sale_num'),\n",
    "         F.avg(\"amount\").alias('avg_sale')) \\\n",
    "    .filter(F.col('total')>20000) \\\n",
    "    .select(F.col('salesperson'), F.col('total'), F.col('sale_num'), F.col('avg_sale'))\n",
    "#tw55.show()\n",
    "\n",
    "t5 = tw55.join(tw5, tw55.salesperson==tw5.sperson) \\\n",
    "    .filter(F.col('total')>20000) \\\n",
    "    .select(F.col('salesperson'), F.col('total'), F.col('sale_num'), F.col('avg_sale'), F.col('product_type').alias('best_product'))\n",
    "t5.show()        \n",
    "\n",
    "s5 = \"\"\"\n",
    "WITH tw5 AS (\n",
    "    SELECT salesperson AS sperson, \n",
    "           product_type,\n",
    "           SUM(amount) AS s_total,\n",
    "           ROW_NUMBER() OVER (PARTITION BY salesperson ORDER BY SUM(amount) DESC) AS rank\n",
    "    FROM sales\n",
    "    GROUP BY salesperson, product_type\n",
    "    ),\n",
    "\n",
    "tw55 AS (\n",
    "    SELECT salesperson,\n",
    "           SUM(amount) AS total,\n",
    "           COUNT(*) AS sale_num,\n",
    "           AVG(amount) AS avg_sale\n",
    "    FROM sales\n",
    "    GROUP BY salesperson\n",
    "    )\n",
    "\n",
    "SELECT salesperson, total, sale_num, avg_sale, product_type AS best_product\n",
    "FROM tw55\n",
    "JOIN tw5 ON tw55.salesperson = tw5.sperson\n",
    "WHERE total > 20000 AND rank=1\n",
    "\"\"\"\n",
    "ts5=spark.sql(s5)\n",
    "ts5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58d7cfb1-95bb-48d3-9d8d-e8bbf88b0f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Intro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
